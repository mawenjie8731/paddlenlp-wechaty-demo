from collections import deque
import os
import asyncio

from wechaty import (
    Contact,
    FileBox,
    Message,
    Wechaty,
    ScanStatus,
)
from wechaty_puppet import MessageType

# # Initialize a PaddleHub plato-mini model
# import paddlehub as hub
# model = hub.Module(name='plato-mini', version='1.0.0')
# model._interactive_mode = True
# model.max_turn = 10
# model.context = deque(maxlen=model.max_turn)


import paddle
import paddle.nn.functional as F
import paddlenlp as ppnlp
from paddlenlp.data import JiebaTokenizer, Pad, Vocab
import numpy as np
from paddlenlp.datasets import load_dataset
from model import TextCNNModel


def preprocess_prediction_data(data, tokenizer, pad_token_id=0, max_ngram_filter_size=3):
    """
    It process the prediction data as the format used as training.
    Args:
        data (obj:`list[str]`): The prediction data whose each element is a tokenized text.
        tokenizer(obj: paddlenlp.data.JiebaTokenizer): It use jieba to cut the chinese string.
        pad_token_id(obj:`int`, optional, defaults to 0): The pad token index.
        max_ngram_filter_size (obj:`int`, optional, defaults to 3) Max n-gram size in TextCNN model.
            Users should refer to the ngram_filter_sizes setting in TextCNN, if ngram_filter_sizes=(1, 2, 3)
            then max_ngram_filter_size=3
    Returns:
        examples (obj:`list`): The processed data whose each element
            is a `list` object, which contains

            - word_ids(obj:`list[int]`): The list of word ids.
    """
    examples = []
    for text in data:
        ids = tokenizer.encode(text)
        seq_len = len(ids)
        # Sequence length should larger or equal than the maximum ngram_filter_size in TextCNN model
        if seq_len < max_ngram_filter_size:
            ids.extend([pad_token_id] * (max_ngram_filter_size - seq_len))
        examples.append(ids)
    return examples

def predict(model, data, label_map, batch_size=1, pad_token_id=0):
    """
    Predicts the data labels.
    Args:
        model (obj:`paddle.nn.Layer`): A model to classify texts.
        data (obj:`list`): The processed data whose each element
            is a `list` object, which contains

            - word_ids(obj:`list[int]`): The list of word ids.
        label_map(obj:`dict`): The label id (key) to label str (value) map.
        batch_size(obj:`int`, defaults to 1): The number of batch.
        pad_token_id(obj:`int`, optional, defaults to 0): The pad token index.
    Returns:
        results(obj:`dict`): All the predictions labels.
    """

    # Seperates data into some batches.
    batches = [
        data[idx:idx + batch_size] for idx in range(0, len(data), batch_size)
    ]
    batchify_fn = lambda samples, fn=Pad(
        axis=0, pad_val=pad_token_id
    ): [data for data in fn(samples)]

    results = []
    model.eval()
    for batch in batches:
        texts = paddle.to_tensor(batchify_fn(batch))
        logits = model(texts)
        probs = F.softmax(logits, axis=1)
        idx = paddle.argmax(probs, axis=1).numpy()
        idx = idx.tolist()
        labels = [label_map[i] for i in idx]
        results.extend(labels)
    return results

# Load vocab.
vocab = Vocab.load_vocabulary(
    './checkpoints/robot_chat_word_dict.txt', unk_token='[UNK]', pad_token='[PAD]')
label_map = {2: 'negative', 1: 'neutral', 0: 'positive'}

# Construct the newtork.
vocab_size = len(vocab)
num_classes = len(label_map)
pad_token_id = vocab.to_indices('[PAD]')

model = TextCNNModel(
    vocab_size,
    num_classes,
    padding_idx=pad_token_id,
    ngram_filter_sizes=(1, 2, 3))

# Load model parameters.
state_dict = paddle.load('./checkpoints/textcnn.pdparams')
model.set_dict(state_dict)

tokenizer = JiebaTokenizer(vocab)


async def on_message(msg: Message):
    """
    Message Handler for the Bot
    """
    ### PaddleHub chatbot
    if isinstance(msg.text(), str) and len(msg.text()) > 0 \
        and msg._payload.type == MessageType.MESSAGE_TYPE_TEXT \
        and msg.text().startswith('[Test]'):  # Use a special token '[Test]' to select messages to respond.
        input_text = msg.text().replace('[Test]', '')
        examples = preprocess_prediction_data([input_text], tokenizer, pad_token_id)
        print("example:",examples)
        results = predict(
            model,
            examples,
            label_map=label_map,
            batch_size=1,
            pad_token_id=pad_token_id)

        result = results[0]
        print('result', result)
        # if isinstance(result, str) == False:
        #     result = "neural"
        # bot_response = model.predict(data=msg.text().replace('[Test]', ''))[0]
        await msg.say(result)  # Return the text generated by PaddleHub chatbot
    ###


async def on_scan(
        qrcode: str,
        status: ScanStatus,
        _data,
):
    """
    Scan Handler for the Bot
    """
    print('Status: ' + str(status))
    print('View QR Code Online: https://wechaty.js.org/qrcode/' + qrcode)


async def on_login(user: Contact):
    """
    Login Handler for the Bot
    """
    print(user)
    # TODO: To be written


async def main():
    """
    Async Main Entry
    """
    #
    # Make sure we have set WECHATY_PUPPET_SERVICE_TOKEN in the environment variables.
    #
    if 'WECHATY_PUPPET_SERVICE_TOKEN' not in os.environ:
        print('''
            Error: WECHATY_PUPPET_SERVICE_TOKEN is not found in the environment variables
            You need a TOKEN to run the Python Wechaty. Please goto our README for details
            https://github.com/wechaty/python-wechaty-getting-started/#wechaty_puppet_service_token
        ''')

    bot = Wechaty()

    bot.on('scan',      on_scan)
    bot.on('login',     on_login)
    bot.on('message',   on_message)

    await bot.start()


asyncio.run(main())
